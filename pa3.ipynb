{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzAWGwlV8lAO"
      },
      "source": [
        "# Programming Assignment 3: Recurrent Neural Networks and Transformer\n",
        "\n",
        "### Instructor: Yuanyuan Shi\n",
        "\n",
        "### Teaching Assistants\n",
        "\n",
        "- Yuexin Bian, [yubian@ucsd.edu]\n",
        "- Tz-Ying Wu, [tzw001@ucsd.edu]\n",
        "\n",
        "## Instructions\n",
        "1. This assignment must be completed individually.  \n",
        "2. This notebook contains skeleton code, which should not be modified\n",
        "3. You must run all cells in this notebook and submit this notebook as an .ipynb file.\n",
        "4. You must submit the .ipynb on Gradescope. You must mark each problem on Gradescope in the pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f_Q7Xuu8lAW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I210M_yQ8lAa"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "# Comment it if you are not using mac\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9msycsbG8lAf"
      },
      "source": [
        "## Part1. RNN models for prediction (30 points)\n",
        "### In this part you will\n",
        "### 1. Implement RNNcell (5 points)\n",
        "### 2. Implement a single-layer RNN (5 points)\n",
        "### 3. Implement a RNN-based predictor (5 points)\n",
        "### 4. Implement functions of loss and training (5 points)\n",
        "### 5. Conduct experiments (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_UzXtBR8lAg"
      },
      "source": [
        "### <font size='4' color='orange'>Task 1.1: Implement a RNNCell (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ufz96k_b8lAh"
      },
      "outputs": [],
      "source": [
        "# Documentation of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
        "class RNNCell(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    RNNCell is a single cell that takes x_t and h_{t-1} as input and outputs h_t.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        \"\"\"\n",
        "        Constructor of RNNCell.\n",
        "        \n",
        "        Inputs: \n",
        "        - input_dim: Dimension of the input x_t\n",
        "        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n",
        "        \"\"\"\n",
        "        \n",
        "        # We always need to do this step to properly implement the constructor\n",
        "        super(RNNCell, self).__init__()\n",
        "        \n",
        "        self.linear_x, self.linear_h, self.non_linear = None, None, None  \n",
        "        \n",
        "        ###########################################################################\n",
        "        # TODO: Define the linear transformation layers for x_t and h_{t-1} and   #\n",
        "        # the non-linear layer. You can use tanh here.                            #\n",
        "        ###########################################################################\n",
        "        self.linear_x = nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=True)\n",
        "        self.linear_h = nn.Linear(in_features=hidden_dim, out_features=hidden_dim, bias=True)\n",
        "        self.non_linear = nn.Tanh()\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "    def forward(self, x_cur: torch.Tensor, h_prev: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute h_t given x_t and h_{t-1}.\n",
        "        \n",
        "        Inputs:\n",
        "        - x_cur: x_t, a tensor with the same of BxC, where B is the batch size and \n",
        "          C is the channel dimension.\n",
        "        - h_prev: h_{t-1}, a tensor with the same of BxH, where H is the channel\n",
        "          dimension.\n",
        "        \"\"\"\n",
        "        h_cur = None\n",
        "        ###########################################################################\n",
        "        # TODO: Define the linear transformation layers for x_t and h_{t-1} and   #\n",
        "        # the non-linear layer.                                                   #\n",
        "        ###########################################################################\n",
        "        out_x = self.linear_x(x_cur)\n",
        "        out_h = self.linear_h(h_prev)\n",
        "        h_cur = self.non_linear(out_x + out_h)\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        return h_cur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xliI4ls8lAj",
        "outputId": "13cc16b3-3852-4107-ee7b-f312eef234b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16])\n"
          ]
        }
      ],
      "source": [
        "# Let's run a sanity check of your model\n",
        "x = torch.randn((2, 8))\n",
        "h = torch.randn((2, 16))\n",
        "model = RNNCell(input_dim=8, hidden_dim = 16)\n",
        "y = model(x, h)\n",
        "assert len(y.shape) == 2 and y.shape == (2,16)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnG1qDfj8lAm"
      },
      "source": [
        "### <font size='4' color='orange'>Task 1.2: Implement a single-layer (single-stack) RNN (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mOf1bx0m8lAo"
      },
      "outputs": [],
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    RNN is a single-layer (stack) RNN by connecting multiple RNNCell together in a single\n",
        "    direction, where the input sequence is processed from left to right.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        \"\"\"\n",
        "        Constructor of the RNN module.\n",
        "        \n",
        "        Inputs: \n",
        "        - input_dim: Dimension of the input x_t\n",
        "        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        ###########################################################################\n",
        "        # TODO: Define the RNNCell.                                               #\n",
        "        ###########################################################################\n",
        "\n",
        "        self.rnn_cell = RNNCell(input_dim, hidden_dim)\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the hidden representations for every token in the input sequence.\n",
        "        \n",
        "        Input:\n",
        "        - x: A tensor with the shape of BxLxC, where B is the batch size, L is the squence \n",
        "          length, and C is the channel dimmension\n",
        "          \n",
        "        Return:\n",
        "        - h: A tensor with the shape of BxLxH, where H is the hidden dimension of RNNCell\n",
        "        \"\"\"\n",
        "        b = x.shape[0]\n",
        "        seq_len = x.shape[1]\n",
        "        \n",
        "        # initialize the hidden dimension\n",
        "        init_h = x.new_zeros((b, self.hidden_dim))\n",
        "        \n",
        "        h = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the hidden representation for every token in the input    #\n",
        "        # from left to right.\n",
        "        ###########################################################################\n",
        "        h = torch.zeros((b, seq_len, self.hidden_dim))\n",
        "        for i in range(seq_len):\n",
        "            h[:, i, :] = self.rnn_cell(x[:, i, :], init_h)\n",
        "            init_h = h[:, i, :]\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        return h\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu9HMihq8lAs",
        "outputId": "39c4090c-f421-45bf-b9e2-a5067f9ec5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 16])\n"
          ]
        }
      ],
      "source": [
        "# Let's run a sanity check of your model\n",
        "x = torch.randn((2, 10, 8))\n",
        "model = RNN(input_dim=8, hidden_dim=16)\n",
        "y = model(x)\n",
        "assert len(y.shape) == 3 and y.shape == (2,10,16)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIu9LrcV8lAv"
      },
      "source": [
        "### <font size='4' color='orange'>Task 1.3: Implement a RNN-based predictor (5 points)</font>\n",
        "### use the final hidden state to to generate the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nhepeC6i8lAx"
      },
      "outputs": [],
      "source": [
        "class RNNmodel(nn.Module):\n",
        "    \"\"\"\n",
        "    A RNN-based predictor\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "            input_dim: int, rnn_hidden_dim: int, output_dim: int\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Constructor.\n",
        "        \n",
        "        Inputs:\n",
        "        - input_dim: input dimension of the sequence\n",
        "        - rnn_hidden_dim: The hidden dimension of the RNN.\n",
        "        - output_dim: output dimension\n",
        "        - pred_type: 'all' or 'last'\n",
        "        \"\"\"\n",
        "        super(RNNmodel, self).__init__()\n",
        "        self.rnn, self.fc = None, None\n",
        "        \n",
        "        ###########################################################################\n",
        "        # TODO: Define the RNN and the predictor layer.                           #\n",
        "        ###########################################################################\n",
        "\n",
        "        self.rnn = RNN(input_dim, rnn_hidden_dim)\n",
        "        self.fc = nn.Linear(rnn_hidden_dim, output_dim)\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Get the prediction result of the input based on the predictor type\n",
        "        \n",
        "        Input:\n",
        "        - x: Tensor with the shape of BxLxC.\n",
        "        \n",
        "        Return:\n",
        "        - y: Tensor with the shape of BxO, where O is the output dimension\n",
        "        \"\"\"\n",
        "        \n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute logits of the input.                                      #\n",
        "        ###########################################################################\n",
        "\n",
        "        out_rnn = self.rnn(x)\n",
        "        y = self.fc(out_rnn[:, -1, :]) #use only last layer \n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNze20y_8lAy",
        "outputId": "4c52823b-3e64-41e9-bdae-6ac869eff241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape: torch.Size([2, 5, 4])\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Let's run a sanity check of your model\n",
        "input_dim = 4\n",
        "length = 5\n",
        "batch_size = 2\n",
        "rnn_hidden_dim = 10\n",
        "output_dim = 2\n",
        "\n",
        "x = torch.randn(batch_size, length, input_dim)\n",
        "print('x.shape: {}'.format(x.shape))\n",
        "model = RNNmodel(input_dim, rnn_hidden_dim, output_dim)\n",
        "y = model(x)\n",
        "assert y.shape == (batch_size, output_dim)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBNdn6fv8lAz"
      },
      "source": [
        "### <font size='4' color='orange'>Task 1.4: Functions of training for a single epoch and evaluation (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s9VvC8Fy8lA2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(model, dataloader, loss_func, device, grad_norm_clip):\n",
        "    model.train()\n",
        "    total_mse, total_count = 0, 0\n",
        "    log_interval = 100\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_pred = None\n",
        "        ###########################################################################\n",
        "        # TODO: compute the prediction of the input, get the loss, and do the     #\n",
        "        # gradient backpropagation.\n",
        "        ###########################################################################\n",
        "\n",
        "        y_pred = model(X)\n",
        "        loss = loss_func(y_pred, y)\n",
        "        loss.backward()\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        optimizer.step()\n",
        "        total_mse += loss.item()\n",
        "        total_count += X.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| MSE {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_mse/total_count))\n",
        "            total_mse, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model, dataloader, loss_func, device, normalize=True):\n",
        "    \"\"\"\n",
        "    If you normalize the data, you should process the prediction and data into raw value and measure the MSE\n",
        "    Normalize function is provided in the following ''createSamples'' function. \n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_mse, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (X, y) in enumerate(dataloader):\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            ###########################################################################\n",
        "            # TODO: compute the prediction of the input, get the loss.                #\n",
        "            # If normlize the data, multiply the prediction value and true value by its range\n",
        "            ###########################################################################\n",
        "            data_min = min(data_min, torch.min(X).item(), torch.min(y).item())\n",
        "            data_max = max(data_max, torch.max(X).item(), torch.max(y).item())\n",
        "            if normalize:\n",
        "                y_pred = model(X)\n",
        "                data_range = dataloader.dataset.dataset_range\n",
        "                data_mean = dataloader.dataset.dataset_mean\n",
        "                y_pred = y_pred * data_range + data_mean\n",
        "                y = y * data_range + data_mean\n",
        "            else:\n",
        "                y_pred = model(X)\n",
        "\n",
        "            loss = loss_func(y_pred, y)\n",
        "            ###########################################################################\n",
        "            #                             END OF YOUR CODE                            #\n",
        "            ###########################################################################\n",
        "            \n",
        "            total_mse += loss.item()\n",
        "            total_count += X.size(0)\n",
        "    return total_mse/total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L15pen5C8lBD"
      },
      "source": [
        "### <font size='4' color='orange'>Task 1.5: Load data and train the model (10 points)</font>\n",
        "### You are expected to achieve < 550 validate MSE for non-normalized data using RNN model\n",
        "### You are expected to achieve < 1 validate MSE for normalized data using RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "fyFm0fBC8lBH",
        "outputId": "902e3219-88da-4710-8750-963d7e33a07b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1475de3ffe78>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'simulation_data_pa3.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'simulation_data_pa3.npz'"
          ]
        }
      ],
      "source": [
        "def plot(data):\n",
        "    fig, ax = plt.subplots(3,2, figsize=(12, 6))\n",
        "    temp = data['temp'] \n",
        "    co2 = data['co2']\n",
        "    people = data['d']\n",
        "    u1 = data['u1']\n",
        "    u2 = data['u2']\n",
        "    amb = data['amb']\n",
        "    show = [0,5]\n",
        "    for idx in range(2):\n",
        "        ax[0][idx].plot(temp[show[idx]], label='temperature', color='#8cc1e6', linewidth=3)\n",
        "        ax[0][idx].plot(amb[show[idx]], label='Ambient temperature', linestyle='--', color='#4069e1', linewidth=3)\n",
        "        ax[0][idx].plot(np.ones(288)*u1[show[idx]], label='Air Flow temperatrue', linestyle='--', color='#4069e1', linewidth=1)\n",
        "        ax[1][idx].plot(co2[show[idx]], label='Indoor CO2',  color='green', linewidth=3)\n",
        "        ax[2][idx].bar(list(range(288)), people[show[idx]], width=1.0, label='People')\n",
        "        ax[1][idx].set_ylim([400, 1400])\n",
        "        ax[2][idx].set_ylim([0, 50])\n",
        "        ax[0][idx].set_ylim([16,24])\n",
        "    for i in range(3):\n",
        "        for idx in range(2):\n",
        "            ax[i][idx].legend()\n",
        "    plt.savefig('data.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "data = np.load('simulation_data_pa3.npz')\n",
        "\n",
        "\n",
        "plot(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh_Nw6Kw8lBM"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
        "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
        "BoolTensor = torch.cuda.BoolTensor if use_cuda else torch.BoolTensor\n",
        "Tensor = FloatTensor\n",
        "\n",
        "def createSamples(data, lookBack, normalize=False):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    temp, co2, people, u1, u2, amb = data['temp'], data['co2'], data['d'], data['u1'], data['u2'], data['amb']\n",
        "    if normalize:\n",
        "        # divide variable of its range\n",
        "        temp /= 30\n",
        "        co2 /= 2000\n",
        "        people /= 100\n",
        "        u1 /= 30\n",
        "        u2 /= 20\n",
        "        amb /= 30\n",
        "    B, T = temp.shape\n",
        "    u1 = np.repeat(u1, T, axis=1)\n",
        "    u2 = np.repeat(u2, T, axis=1)\n",
        "    for t in range(T-lookBack):\n",
        "        x = np.stack([temp[:,t:t+lookBack], co2[:,t:t+lookBack], people[:,t:t+lookBack], u1[:,t:t+lookBack], u2[:,t:t+lookBack], amb[:,t:t+lookBack]], axis=-1)\n",
        "        y = np.concatenate([temp[:,t+lookBack:t+lookBack+1], co2[:,t+lookBack:t+lookBack+1]], axis=1)\n",
        "        dataX.append(x)\n",
        "        dataY.append(y)\n",
        "    X = np.concatenate(dataX, axis=0)\n",
        "    Y = np.concatenate(dataY, axis=0)\n",
        "    return Tensor(X), Tensor(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Niq1msPv8lBQ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# TODO: try different Hyper parameters\n",
        "epochs = 5 # epoch\n",
        "lr = 1e-4 # learning rate\n",
        "batch_size = 64 # batch size for training\n",
        "rnn_hidden_dim = 10\n",
        "input_dim = 6\n",
        "output_dim = 2\n",
        "\n",
        "model, loss_func = None, None\n",
        "###########################################################################\n",
        "# TODO: Deinfe the classifier and loss function.\n",
        "###########################################################################\n",
        "model = RNNmodel(input_dim, rnn_hidden_dim, output_dim)\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "total_mse = None\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-2)\n",
        "\n",
        "# Change the normalize parameters here to conduct experiment for normalized data or the raw data\n",
        "X, y = createSamples(data, lookBack=24, normalize=False)\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for epoch in range(1, epochs + 1, 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, train_dataloader, loss_func, device, 1)\n",
        "    mse_val = evaluate(model, valid_dataloader, loss_func, device)\n",
        "    if total_mse is not None and total_mse > mse_val:\n",
        "        scheduler.step()\n",
        "    else:\n",
        "        total_mse = mse_val\n",
        "    if epoch == 5000:\n",
        "        optimizer.param_groups[0]['lr'] = 5e-2\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid mse {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           mse_val))\n",
        "    #print('-' * 59)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUeyq_x8lBT"
      },
      "source": [
        "## Part 2: Transformer predictor (50 points)\n",
        "### In this part you will\n",
        "### 1. Implement Multi-head attention (5 points)\n",
        "### 2. Implement a feed-forward NN (5 points)\n",
        "### 3. Implement a Single Transformer Encoder Cell (5 points)\n",
        "### 4. Implement Transformer Encoder (5 points)\n",
        "### 5. Implement Positional Encoding (10 points)\n",
        "### 6. Implement a Transformer-based predictor (5 points)\n",
        "### 7. Conduct experiments (10 points)\n",
        "### 8. Discuss (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmlRgCgt8lBW"
      },
      "source": [
        "### <font size='4' color='orange'>Task 2.1: Implement the multi-head attention module (no for loops allowed, 5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IsK7vHz8lBX"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A module that computes multi-head attention given query, key, and value tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int):\n",
        "        \"\"\"\n",
        "        Constructor.\n",
        "        \n",
        "        Inputs:\n",
        "        - input_dim: Dimension of the input query, key, and value. Here we assume they all have\n",
        "          the same dimensions. But they could have different dimensions in other problems.\n",
        "        - num_heads: Number of attention heads\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        \n",
        "        assert input_dim % num_heads == 0\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_per_head = input_dim // num_heads\n",
        "        \n",
        "        ###########################################################################\n",
        "        # TODO: Define the linear transformation layers for key, value, and query.#\n",
        "        # Also define the output layer.\n",
        "        ###########################################################################\n",
        "        # K, Q, V\n",
        "\n",
        "        self.K = nn.Linear(self.input_dim, self.input_dim)\n",
        "        self.Q = nn.Linear(self.input_dim, self.input_dim)\n",
        "        self.V = nn.Linear(self.input_dim, self.input_dim)\n",
        "        self.linear_out = nn.Linear(self.input_dim, self.input_dim)\n",
        "\n",
        "        # output layer\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        \n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None):\n",
        "        \"\"\"\n",
        "        Compute the attended feature representations.\n",
        "        \n",
        "        Inputs:\n",
        "        - query: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "          and C is the channel dimension\n",
        "        - key: Tensor of the shape BxLxC\n",
        "        - value: Tensor of the shape BxLxC\n",
        "        - mask: Tensor indicating where the attention should *not* be performed\n",
        "        \"\"\"\n",
        "        b = query.shape[0]        \n",
        "        \n",
        "        dot_prod_scores = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the scores based on dot product between transformed query,#\n",
        "        # key, and value. You may find torch.matmul helpful, whose documentation  #\n",
        "        # can be found at                                                         #\n",
        "        # https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul#\n",
        "        # Remember to devide the doct product similarity scores by square root of #\n",
        "        # the channel dimension per head.   \n",
        "        #                                                                         #\n",
        "        # Since no for loops are allowed here, think of how to use tensor reshape #\n",
        "        # to process multiple attention heads at the same time.                   #\n",
        "        ###########################################################################\n",
        "        # create K, Q, V\n",
        "\n",
        "        q = self.Q(query).reshape(b, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
        "        k = self.K(key).reshape(b, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
        "        v = self.V(value).reshape(b, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
        "        # compute attention scores\n",
        "        dot_prod_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.dim_per_head, dtype=torch.float32))\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        if mask is not None:\n",
        "            # We simply set the similarity scores to be near zero for the positions\n",
        "            # where the attention should not be done. Think of why we do this.\n",
        "            dot_prod_scores = dot_prod_scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        out = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the attention scores, which are then used to modulate the #\n",
        "        # value tensor. Finally concate the attended tensors from multiple heads  #\n",
        "        # and feed it into the output layer. You may still find torch.matmul      #\n",
        "        # helpful.                                                                #\n",
        "        #                                                                         #\n",
        "        # Again, think of how to use reshaping tensor to do the concatenation.    #\n",
        "        ###########################################################################\n",
        "        \n",
        "        # get attended features\n",
        "        attention_weights = torch.softmax(dot_prod_scores, dim=-1)  # BxHxLxL\n",
        "        # concatenate the output from different heads\n",
        "        attended_values = torch.matmul(attention_weights, v)\n",
        "        attended_values = attended_values.transpose(1, 2).contiguous().view(b, seq_len, -1)\n",
        "        # output\n",
        "        out = self.linear_out(attended_values)\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5rrE3Av8lBY"
      },
      "outputs": [],
      "source": [
        "# Let's run a sanity check of your model\n",
        "x = torch.randn((2, 10, 8))\n",
        "mask = torch.randn((2, 10)) > 0.5\n",
        "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
        "num_heads = 4\n",
        "model = MultiHeadAttention(8, num_heads)\n",
        "y = model(x, x, x, mask)\n",
        "assert len(y.shape) == len(x.shape)\n",
        "for dim_x, dim_y in zip(x.shape, y.shape):\n",
        "    assert dim_x == dim_y\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-GWDYG_8lBa"
      },
      "source": [
        "### <font size='4' color='orange'>Task 2.2: Implement a Feedforward Network (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuGk9d5n8lBa"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feedforward network. Essentially, it is a two-layer fully-connected\n",
        "    neural network.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, ff_dim):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension\n",
        "        - ff_dim: Hidden dimension\n",
        "        \"\"\"\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        \n",
        "        ###########################################################################\n",
        "        # TODO: Define the two linear layers and a non-linear one.\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "         and C is the channel dimension\n",
        "          \n",
        "        Return:\n",
        "        - y: Tensor of the shape BxLxC\n",
        "        \"\"\"\n",
        "        \n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Process the input.                                                #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        return x\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-Utfl5g8lBe"
      },
      "outputs": [],
      "source": [
        "# Let's run a sanity check of your model\n",
        "x = torch.randn((2, 10, 8))\n",
        "ff_dim = 4\n",
        "model = FeedForwardNetwork(8, ff_dim, 0.1)\n",
        "y = model(x)\n",
        "assert len(x.shape) == len(y.shape)\n",
        "for dim_x, dim_y in zip(x.shape, y.shape):\n",
        "    assert dim_x == dim_y\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwccGUbv8lBg"
      },
      "source": [
        "### <font size='4' color='orange'>Task 2.3: Implement a Single Transformer Encoder Cell (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFiEesUj8lBg"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderCell(nn.Module):\n",
        "    \"\"\"\n",
        "    A single cell (unit) for the Transformer encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension for each token in a sequence\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - ff_dim: The hidden dimension for a feedforward network\n",
        "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
        "          modules.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoderCell, self).__init__()\n",
        "        \n",
        "        ###########################################################################\n",
        "        # TODO: A single Transformer encoder cell consists of \n",
        "        # 1. A multi-head attention module\n",
        "        # 2. Followed by dropout\n",
        "        # 3. Followed by layer norm (check nn.LayerNorm)\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm\n",
        "        \n",
        "        # At the same time, it also has\n",
        "        # 1. A feedforward network\n",
        "        # 2. Followed by dropout\n",
        "        # 3. Followed by layer norm\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "          and C is the channel dimension\n",
        "        - mask: Tensor for multi-head attention\n",
        "        Returns:\n",
        "        - y: Tensor of the shape BxLxC\n",
        "        \"\"\"\n",
        "        \n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Get the output of the multi-head attention part (with dropout     #\n",
        "        # and layer norm), which is used as input to the feedforward network (    #\n",
        "        # again, followed by dropout and layer norm).                             #\n",
        "        #                                                                         #\n",
        "        # Don't forget the residual connections for both parts.                   #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfRk5k4X8lBm"
      },
      "outputs": [],
      "source": [
        "# Let's run a sanity check of your model\n",
        "mask = torch.randn((2, 10)) > 0.5\n",
        "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
        "num_heads = 4\n",
        "model = TransformerEncoderCell(8, num_heads, 32, 0.1)\n",
        "y = model(x, mask)\n",
        "assert len(x.shape) == len(y.shape)\n",
        "for dim_x, dim_y in zip(x.shape, y.shape):\n",
        "    assert dim_x == dim_y\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqQA_PnT8lBo"
      },
      "source": [
        "### <font size='4' color='orange'>Task 2.4: Implement Transformer Encoder (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61rS4ZVG8lBp"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A full encoder consisting of a set of TransformerEncoderCell.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout: float=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension for each token in a sequence\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - ff_dim: The hidden dimension for a feedforward network\n",
        "        - num_cells: Number of TransformerEncoderCells\n",
        "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
        "          modules.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        \n",
        "        self.norm = None\n",
        "        ###########################################################################\n",
        "        # TODO: Construct a nn.ModuleList to store a stack of                     #\n",
        "        # TranformerEncoderCells. Check the documentation here of how to use it   #\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n",
        "        \n",
        "        # At the same time, define a layer normalization layer to process the     #\n",
        "        # output of the entire encoder.                                           #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "          and C is the channel dimension\n",
        "        - mask: Tensor for multi-head attention\n",
        "        \n",
        "        Return:\n",
        "        - y: Tensor of the shape of BxLxC, which is the normalized output of the encoder\n",
        "        \"\"\"\n",
        "        \n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Feed x into the stack of TransformerEncoderCells and then         #\n",
        "        # normalize the output with layer norm.                                   #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        return y\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA0L9XwZ8lBs"
      },
      "outputs": [],
      "source": [
        "# Let's run a sanity check of your model\n",
        "x = torch.randn((2, 10, 8))\n",
        "mask = torch.randn((2, 10)) > 0.5\n",
        "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
        "num_heads = 4\n",
        "model = TransformerEncoder(8, num_heads, 32, 2, 0.1)\n",
        "y = model(x)\n",
        "assert len(x.shape) == len(y.shape)\n",
        "for dim_x, dim_y in zip(x.shape, y.shape):\n",
        "    assert dim_x == dim_y\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsA7kUPn8lBy"
      },
      "source": [
        "### <font size='4' color='orange'>Task 2.5: Implement Positional Encoding (10 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqO4keh58lBz"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    A module that adds positional encoding to each of the token's features.\n",
        "    So that the Transformer is position aware.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, max_len: int=10000):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension about the features for each token\n",
        "        - max_len: The maximum sequence length\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the positional encoding and add it to x.\n",
        "        \n",
        "        Input:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "          and C is the channel dimension\n",
        "          \n",
        "        Return:\n",
        "        - x: Tensor of the shape BxLxC, with the positional encoding added to the input\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        input_dim = x.shape[2]\n",
        "        \n",
        "        pe = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the positional encoding                                   #\n",
        "        # Check Section 3.5 for the definition (https://arxiv.org/pdf/1706.03762.pdf)\n",
        "        #                                                                         #\n",
        "        # It's a bit messy, but the definition is provided for your here for your #\n",
        "        # convenience (in LaTex).                                                 #\n",
        "        # PE_{(pos,2i)} = sin(pos / 10000^{2i/\\dmodel}) \\\\                        #\n",
        "        # PE_{(pos,2i+1)} = cos(pos / 10000^{2i/\\dmodel})                         #\n",
        "        #                                                                         #\n",
        "        # You should replace 10000 with max_len here.\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        x = x + pe.to(x.device)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HSUPvII8lB0"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "pe = PositionalEncoding(20, 0)\n",
        "y = pe.forward((torch.zeros(1, 100, 20)))\n",
        "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
        "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkAn5R6l8lB0"
      },
      "source": [
        "### <font size='4' color='orange'>Task 2.6: Implement a Transformer-based predictor (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQR5ahnQ8lB1"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer-based predictor.\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "           input_dim: int, num_heads:int, trx_ff_dim: int, num_trx_cells:int, output_dim: int, dropout: float=0.1\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: input_dimension of the sequence\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - trx_ff_dim: The hidden dimension for a feedforward network\n",
        "        - num_trx_cells: Number of TransformerEncoderCells\n",
        "        - output_dim: output dimension\n",
        "        - dropout: Dropout ratio\n",
        "        \"\"\"\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        ###########################################################################\n",
        "        # TODO: Define a module for positional encoding, Transformer encoder, and #\n",
        "        # a output layer                                                          #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - x: Tensor with the shape of BxLxC.\n",
        "        - mask: Tensor for multi-head attention\n",
        "        \n",
        "        Return:\n",
        "        - y: Tensor with the shape of BxK, where K is the output dimension\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Apply positional embedding to the input, which is then fed into   #\n",
        "        # the encoder. Average pooling is applied then to all the features of all #\n",
        "        # tokens. Finally, the prediction value are computed based on the pooled features.  #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        \n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL8mKN458lB3"
      },
      "outputs": [],
      "source": [
        "# Let's run a sanity check of your model\n",
        "batch_size, input_dim, output_dim, sequence_l = 2, 6, 3, 10\n",
        "num_heads = 3\n",
        "trx_ff_dim = 16\n",
        "num_trx_cells = 2\n",
        "\n",
        "x = torch.randn(batch_size, sequence_l, input_dim)\n",
        "mask = None\n",
        "model = TransformerModel(input_dim=input_dim, num_heads=num_heads, trx_ff_dim=trx_ff_dim, num_trx_cells=num_trx_cells, output_dim=output_dim)\n",
        "print('x: {}'.format(x.shape))\n",
        "y = model(x, mask)\n",
        "assert len(y.shape) == 2 and y.shape[0] == x.shape[0] and y.shape[1] == output_dim\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN_tMTrj8lB3"
      },
      "source": [
        "### <font size='4' color='orange'>Task 2.7: Conduct experiments here</font>\n",
        "### You are expected to achieve < 50 validate MSE for non-normalized data\n",
        "### You are expected to achieve < 1 validate MSE for normalized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRKglMZ-8lB4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Try Hyper parameters\n",
        "epochs = 100 # epoch\n",
        "lr = 1e-3 # learning rate\n",
        "batch_size = 64 # batch size for training\n",
        "num_heads = 3\n",
        "num_trx_cells = 2\n",
        "input_dim = 6\n",
        "output_dim = 2\n",
        "trx_ff_dim = 30\n",
        "\n",
        "model, loss_func = None, None\n",
        "###########################################################################\n",
        "# TODO: Deinfe the classifier and loss function.\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "total_mse = None\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-2)\n",
        "\n",
        "# Change the normalize parameters here to conduct experiment for normalized data or the raw data\n",
        "X, y = createSamples(data, lookBack=24, normalize=True)\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for epoch in range(1, epochs + 1, 100):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, train_dataloader, loss_func, device, 1)\n",
        "    mse_val = evaluate(model, valid_dataloader, loss_func, device)\n",
        "    if total_mse is not None and total_mse > mse_val:\n",
        "        scheduler.step()\n",
        "    else:\n",
        "        total_mse = mse_val\n",
        "    if epoch == 100:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-3\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid mse {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           mse_val))\n",
        "    #print('-' * 59)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "78c8e1b80b1031a1e136770481d214899efde752997b0b94966a80ddc7ada738"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}